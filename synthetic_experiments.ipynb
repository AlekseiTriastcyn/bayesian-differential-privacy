{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from bayesian_privacy_accountant import BayesianPrivacyAccountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_eps = []\n",
    "ba_eps = []\n",
    "\n",
    "quant = 0.05\n",
    "sigma = 1.0\n",
    "plot_range = np.arange(100)\n",
    "\n",
    "moment_accountant = BayesianPrivacyAccountant(powers=16, total_steps=plot_range[-1]+1, bayesianDP=False)\n",
    "bayes_accountant = BayesianPrivacyAccountant(powers=16, total_steps=plot_range[-1]+1)\n",
    "for i in plot_range:\n",
    "    grads = np.random.weibull(0.5, [50, 1000])\n",
    "    C = np.quantile(np.linalg.norm(grads, axis=1), quant)\n",
    "    #grads /= np.maximum(1, np.linalg.norm(grads, axis=1, keepdims=True) / C)\n",
    "    \n",
    "    moment_accountant.accumulate(\n",
    "        ldistr=(C*2, sigma * C * 2), # multiply by 2, because 2C is the actual max distance\n",
    "        rdistr=(0, sigma * C * 2),\n",
    "        q=1/1000, \n",
    "        steps=1\n",
    "    )\n",
    "    ma_eps += [moment_accountant.get_privacy(target_delta=1e-5)[0]]\n",
    "    pairs = list(zip(*itertools.combinations(torch.tensor(grads), 2)))\n",
    "    bayes_accountant.accumulate(\n",
    "        ldistr=(torch.stack(pairs[0]), sigma * C * 2), \n",
    "        rdistr=(torch.stack(pairs[1]), sigma * C * 2), \n",
    "        #ldistr=(torch.tensor(grads[:25]), sigma * C * 2), \n",
    "        #rdistr=(torch.tensor(grads[25:]), sigma * C * 2), \n",
    "        q=1/1000, \n",
    "        steps=1\n",
    "    ) \n",
    "    ba_eps += [bayes_accountant.get_privacy(target_delta=1e-5)[0]]\n",
    "\n",
    "plt.plot(plot_range, ma_eps, label='DP')\n",
    "plt.plot(plot_range, ba_eps, label='BDP')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel(r'Step', fontsize=12)\n",
    "plt.ylabel(r'$\\varepsilon$', fontsize=12)\n",
    "plt.title(r'Privacy loss evolution, $C=0.05$-quantile, no clipping', fontsize=12)\n",
    "plt.savefig('eps_step_05q_noclip.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.random.weibull(0.5, [50, 1000])\n",
    "\n",
    "quant = 0.99\n",
    "C = np.quantile(np.linalg.norm(grads, axis=1), quant)\n",
    "grads /= np.maximum(1, np.linalg.norm(grads, axis=1, keepdims=True) / C)\n",
    "\n",
    "ma_eps = []\n",
    "ba_eps = []\n",
    "sigmas = np.arange(0.55, 1.4, 0.05)\n",
    "for sigma in sigmas:\n",
    "    moment_accountant = BayesianPrivacyAccountant(powers=24, total_steps=1, bayesianDP=False)\n",
    "    moment_accountant.accumulate(ldistr=(C * 2, sigma * C * 2), rdistr=(0, sigma * C * 2), q=1/1000, steps=1)\n",
    "    ma_eps += [moment_accountant.get_privacy(target_delta=1e-5)[0]]\n",
    "    \n",
    "    bayes_accountant = BayesianPrivacyAccountant(powers=24, total_steps=1)\n",
    "    pairs = list(zip(*itertools.combinations(torch.tensor(grads), 2)))\n",
    "    bayes_accountant.accumulate(\n",
    "        ldistr=(torch.stack(pairs[0]), sigma * C * 2), \n",
    "        rdistr=(torch.stack(pairs[1]), sigma * C * 2), \n",
    "        q=1/1000, \n",
    "        steps=1\n",
    "    ) \n",
    "    ba_eps += [bayes_accountant.get_privacy(target_delta=1e-5)[0]]\n",
    "\n",
    "plt.rc('axes', titlesize=14, labelsize=14)\n",
    "plt.plot(sigmas, ma_eps, '--', linewidth=2, label='DP')\n",
    "plt.plot(sigmas, ba_eps, linewidth=2, label='BDP')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\sigma$')\n",
    "plt.ylabel(r'$\\varepsilon$')\n",
    "plt.title(r'$\\sigma$ vs $\\varepsilon$, $C=0.99$-quantile')\n",
    "plt.savefig('eps_sigma_99q.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_eps = []\n",
    "\n",
    "sigma = 1.0\n",
    "plot_range = range(1, 50)\n",
    "\n",
    "ba_eps_c = []\n",
    "for C in [0.1, 0.7, 1.0]:\n",
    "    grads = np.random.randn(50, 1000)\n",
    "    grads /= np.maximum(1, np.linalg.norm(grads, axis=1, keepdims=True) / C)\n",
    "    ba_eps = []\n",
    "    for p in plot_range: \n",
    "        bayes_accountant = BayesianPrivacyAccountant(powers=p, total_steps=10000, bayesianDP=False)\n",
    "        bayes_accountant.accumulate(ldistr=(C, sigma), rdistr=(0, sigma), q=64/60000, steps=10000)\n",
    "        ba_eps += [bayes_accountant.get_privacy(target_delta=1e-5)[0]]\n",
    "    ba_eps_c += [ba_eps]\n",
    "\n",
    "plt.rc('axes', titlesize=14, labelsize=14)\n",
    "plt.plot(plot_range, ba_eps_c[0], ':', linewidth=2, label='C=0.1')\n",
    "plt.plot(plot_range, ba_eps_c[1], '--', linewidth=2, label='C=0.7')\n",
    "plt.plot(plot_range, ba_eps_c[2], '-', linewidth=2, label='C=1.0')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(r'$\\varepsilon$')\n",
    "plt.title(r'$\\lambda$ vs $\\varepsilon$ for different C, $\\delta=10^{-5}$')\n",
    "plt.savefig('eps_lambda_C.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
