{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.optim.optimizer import required\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "from bayesian_privacy_accountant import BayesianPrivacyAccountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='mnist', help='mnist | cifar10 | svhn')\n",
    "parser.add_argument('--dataroot', default='data', help='path to dataset')\n",
    "parser.add_argument('--batchSize', type=int, default=1024, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=28, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nClasses', type=int, default=10, help='number of labels (classes)')\n",
    "parser.add_argument('--nChannels', type=int, default=1, help='number of colour channels')\n",
    "parser.add_argument('--ndf', type=int, default=64, help='number of filters in CNN')\n",
    "parser.add_argument('--n_epochs', type=int, default=32, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--C', type=float, default=1.0, help='embedding L2-norm bound, default=1.0')\n",
    "parser.add_argument('--sigma', type=float, default=0.1, help='noise variance, default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--outf', default='output', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, default=8664, help='manual seed for reproducibility')\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    opt.cuda = True\n",
    "    opt.ngpu = 1\n",
    "    gpu_id = 0\n",
    "    print(\"Using CUDA: gpu_id = %d\" % gpu_id)\n",
    "    \n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements a reshaping module.\n",
    "        Allows to reshape a tensor between NN layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.view(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterSize = 5\n",
    "w_out = 4\n",
    "h_out = 4\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(opt.nChannels, opt.ndf, filterSize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(opt.ndf),\n",
    "            nn.Conv2d(opt.ndf, opt.ndf, filterSize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(opt.ndf),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            View(-1, opt.ndf * w_out * h_out),\n",
    "            #PrintLayer(\"View\"),\n",
    "            #View(-1, 784),\n",
    "            nn.Linear(opt.ndf * w_out * h_out, 384),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Linear(192, opt.nClasses),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(testloader, net):\n",
    "    \"\"\"\n",
    "        Compute test accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    '''\n",
    "    if opt.cuda:\n",
    "        net = net.cuda()\n",
    "    '''\n",
    "    \n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        if opt.cuda:\n",
    "            images = images.cuda(gpu_id)\n",
    "            labels = labels.cuda(gpu_id)\n",
    "            \n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #print(predicted.cpu().numpy())\n",
    "        #print(labels.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == (labels.long().view(-1) % 10)).sum()\n",
    "        #print torch.cat([predicted.view(-1, 1), (labels.long() % 10)], dim=1)\n",
    "\n",
    "    print('Accuracy of the network on test images: %f %%' % (100 * float(correct) / total))\n",
    "    return 100 * float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_update(params, p, use_grad_field=True):\n",
    "    init = True\n",
    "    for param in params:\n",
    "        if param is not None:\n",
    "            if init:\n",
    "                idx = torch.zeros_like(param, dtype=torch.bool)\n",
    "                idx.bernoulli_(1 - p)\n",
    "            if use_grad_field:\n",
    "                if param.grad is not None:\n",
    "                    idx = torch.zeros_like(param, dtype=torch.bool)\n",
    "                    idx.bernoulli_(1 - p)\n",
    "                    param.grad.data[idx] = 0\n",
    "            else:\n",
    "                init = False\n",
    "                param.data[idx] = 0\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, student, n_epochs=25, lr=0.0001, accountant=None):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    #optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "    optimizer = optim.SGD(student.parameters(), lr=lr)\n",
    "    \n",
    "    if opt.cuda:\n",
    "        student = student.cuda(gpu_id)\n",
    "        criterion = criterion.cuda(gpu_id)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    num_batches = len(trainloader.dataset) / opt.batchSize + 1\n",
    "    sampling_prob = 0.1\n",
    "    max_grad_norm = opt.C\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda(gpu_id)\n",
    "                labels = labels.cuda(gpu_id)\n",
    "            \n",
    "            inputv = Variable(inputs)\n",
    "            labelv = Variable(labels.long().view(-1) % 10)\n",
    "            \n",
    "            batch_size = float(len(inputs))\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = student(inputv)\n",
    "            loss = criterion(outputs, labelv)\n",
    "            \n",
    "            #max_grad_norm = opt.C * 0.9**epoch\n",
    "            if accountant:\n",
    "                grads_est = []\n",
    "                num_subbatch = 8\n",
    "                for j in range(num_subbatch):\n",
    "                    grad_sample = torch.autograd.grad(\n",
    "                        loss[np.delete(range(int(batch_size)), j)].mean(), \n",
    "                        [p for p in student.parameters() if p.requires_grad], \n",
    "                        retain_graph=True\n",
    "                    )\n",
    "                    with torch.no_grad():\n",
    "                        grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "                        grad_sample /= max(1.0, grad_sample.norm().item() / max_grad_norm)\n",
    "                        grads_est += [grad_sample]\n",
    "                with torch.no_grad():\n",
    "                    grads_est = torch.stack(grads_est)\n",
    "                    sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "                \n",
    "            (loss.mean()).backward()\n",
    "            running_loss += loss.mean().item()\n",
    "            \n",
    "            if accountant:\n",
    "                with torch.no_grad():\n",
    "                    torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "                    for group in optimizer.param_groups:\n",
    "                        for p in group['params']:\n",
    "                            if p.grad is not None:\n",
    "                                p.grad += torch.randn_like(p.grad) * (opt.sigma * max_grad_norm)\n",
    "                    sparsify_update(student.parameters(), p=sampling_prob)\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            if accountant:\n",
    "                with torch.no_grad():\n",
    "                    batch_size = float(len(inputs))\n",
    "                    q = batch_size / len(trainloader.dataset)\n",
    "                    # NOTE: \n",
    "                    # Using combinations within a set of gradients (like below)\n",
    "                    # does not actually produce samples from the correct distribution\n",
    "                    # (for that, we need to sample pairs of gradients independently).\n",
    "                    # However, the difference is not significant, and it speeds up computations.\n",
    "                    pairs = list(zip(*itertools.combinations(grads_est, 2)))\n",
    "                    accountant.accumulate(\n",
    "                        ldistr=(torch.stack(pairs[0]), opt.sigma*max_grad_norm),\n",
    "                        rdistr=(torch.stack(pairs[1]), opt.sigma*max_grad_norm),\n",
    "                        q=q, \n",
    "                        steps=1,\n",
    "                    )\n",
    "            \n",
    "        # print training stats every epoch\n",
    "        running_eps = accountant.get_privacy(target_delta=1e-5) if accountant else None\n",
    "        print(\"Epoch: %d/%d. Loss: %.3f. Privacy (𝜀,𝛿): %s\" %\n",
    "              (epoch + 1, n_epochs, running_loss / len(trainloader), running_eps))\n",
    "                \n",
    "        acc = test(testloader, student)\n",
    "        accuracies += [acc]\n",
    "        print(\"Test accuracy is %d %%\" % acc)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return student.cpu(), accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations applied to data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# switch datasets\n",
    "if opt.dataset == 'mnist':\n",
    "    trainset = torchvision.datasets.MNIST(root=opt.dataroot + os.sep + opt.dataset, train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.MNIST(root=opt.dataroot + os.sep + opt.dataset, train=False, download=True, transform=transform)\n",
    "elif opt.dataset == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root=opt.dataroot + os.sep + opt.dataset, train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root=opt.dataroot + os.sep + opt.dataset, train=False, download=True, transform=transform)\n",
    "elif opt.dataset == 'svhn':\n",
    "    trainset = torchvision.datasets.SVHN(root=opt.dataroot + os.sep + opt.dataset, split='train', download=True, transform=transform)\n",
    "    testset = torchvision.datasets.SVHN(root=opt.dataroot + os.sep + opt.dataset, split='test', download=True, transform=transform)\n",
    "\n",
    "#val_size = 10000\n",
    "#valset = torch.utils.data.Subset(trainset, range(val_size))\n",
    "#trainset = torch.utils.data.Subset(trainset, range(val_size, len(trainset)))\n",
    "    \n",
    "# initialise data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.batchSize, shuffle=True, num_workers=2, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=opt.batchSize, shuffle=True, num_workers=2)\n",
    "\n",
    "# names of classes\n",
    "classes = tuple(np.arange(10).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "netS = SimpleConvNet()\n",
    "total_steps = opt.n_epochs * len(trainloader)\n",
    "bayes_accountant = BayesianPrivacyAccountant(powers=[2, 4, 8, 16, 32], total_steps=total_steps)\n",
    "netS, accs = train(trainloader, netS, lr=0.02, n_epochs=opt.n_epochs, accountant=bayes_accountant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bayesian DP (𝜀,𝛿): \", bayes_accountant.get_privacy(target_delta=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "sampling_prob = 0.1\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "netS.cuda(gpu_id)\n",
    "loss_fn.cuda(gpu_id)\n",
    "\n",
    "dists_train = []\n",
    "dists_test = []\n",
    "\n",
    "i = 0\n",
    "for inputs, labels in trainloader:\n",
    "    i += 1\n",
    "    if i > 8:\n",
    "        break\n",
    "    inputs = inputs.cuda(gpu_id)\n",
    "    labels = labels.cuda(gpu_id)\n",
    "    netS.zero_grad()\n",
    "    outputs = netS(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    grads_est = []\n",
    "    num_subbatch = 100\n",
    "    for j in range(num_subbatch):\n",
    "        grad_sample = torch.autograd.grad(loss[np.delete(range(int(opt.batchSize)), j)].mean(), [p for p in netS.parameters() if p.requires_grad], retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "            grad_sample /= max(1.0, grad_sample.norm().item() / opt.C)\n",
    "            grads_est += [grad_sample]\n",
    "    with torch.no_grad():\n",
    "        grads_est = torch.stack(grads_est)\n",
    "        sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "    q = opt.batchSize / len(trainloader.dataset)\n",
    "    dists_train += [pdist(grads_est.cpu())]\n",
    "    \n",
    "i = 0\n",
    "for inputs, labels in testloader:\n",
    "    i += 1\n",
    "    if i > 8:\n",
    "        break\n",
    "    inputs = inputs.cuda(gpu_id)\n",
    "    labels = labels.cuda(gpu_id)\n",
    "    netS.zero_grad()\n",
    "    outputs = netS(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    grads_est = []\n",
    "    num_subbatch = 100\n",
    "    for j in range(num_subbatch):\n",
    "        grad_sample = torch.autograd.grad(loss[np.delete(range(int(opt.batchSize)), j)].mean(), [p for p in netS.parameters() if p.requires_grad], retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "            grad_sample /= max(1.0, grad_sample.norm().item() / opt.C)\n",
    "            grads_est += [grad_sample]\n",
    "    with torch.no_grad():\n",
    "        grads_est = torch.stack(grads_est)\n",
    "        sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "    q = opt.batchSize / len(trainloader.dataset)\n",
    "    dists_test += [pdist(grads_est.cpu())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp, ttest_rel\n",
    "\n",
    "dists_train = np.stack(dists_train).squeeze()\n",
    "dists_test = np.stack(dists_test).squeeze()\n",
    "\n",
    "plt.hist(dists_train.flatten(), bins=np.arange(0, 0.02, 0.0005), label='Train', alpha=0.5)\n",
    "plt.hist(dists_test.flatten(), bins=np.arange(0, 0.02, 0.0005), label='Test', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(r'Distance')\n",
    "plt.ylabel(r'Number of samples')\n",
    "plt.title(r'Pairwise gradient distances distribution, MNIST')\n",
    "plt.savefig('grad_dist_histogram_mnist_2.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, ttest_ind, levene\n",
    "\n",
    "print(ttest_rel(dists_train.flatten(), dists_test.flatten()))\n",
    "print(ttest_ind(dists_train.flatten(), dists_test.flatten()))\n",
    "print(levene(dists_train.flatten(), dists_test.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
