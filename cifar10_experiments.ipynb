{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.optim.optimizer import required\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "from bayesian_privacy_accountant import BayesianPrivacyAccountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='cifar10', help='mnist | cifar10 | svhn')\n",
    "parser.add_argument('--dataroot', default='data', help='path to dataset')\n",
    "parser.add_argument('--batchSize', type=int, default=512, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nClasses', type=int, default=10, help='number of labels (classes)')\n",
    "parser.add_argument('--nChannels', type=int, default=3, help='number of colour channels')\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--filterSize', type=int, default=5)\n",
    "parser.add_argument('--n_epochs', type=int, default=2, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--C', type=float, default=1.0, help='embedding L2-norm bound, default=1.0')\n",
    "parser.add_argument('--sigma', type=float, default=0.8, help='noise variance, default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--outf', default='output', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, default=2560, help='manual seed for reproducibility')\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    opt.cuda = True\n",
    "    opt.ngpu = 1\n",
    "    gpu_id = 1\n",
    "    print(\"Using CUDA: gpu_id = %d\" % gpu_id)\n",
    "    \n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements a reshaping module.\n",
    "        Allows to reshape a tensor between NN layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.view(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "filterSize = 5\n",
    "w_out = 5\n",
    "h_out = 5\n",
    "\n",
    "def compute_dim_out(num_conv_layers, dim_in, kernel_size=opt.filterSize, stride=2, padding=0, dilation=1):\n",
    "    dim_out = dim_in\n",
    "    for i in range(num_conv_layers):\n",
    "        dim_out = np.int((dim_out + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n",
    "    return dim_out\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(opt.nChannels, opt.ndf, filterSize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(opt.ndf),\n",
    "            nn.Conv2d(opt.ndf, opt.ndf, filterSize),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(opt.ndf),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            View(-1, opt.ndf * w_out * h_out),\n",
    "            #PrintLayer(\"View\"),\n",
    "            nn.Linear(opt.ndf * w_out * h_out, 384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(384, 384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(384, opt.nClasses),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(student, valset=None):\n",
    "    \"\"\"\n",
    "        Compute test accuracy.\n",
    "    \"\"\"\n",
    "    real_loss = 0\n",
    "    real_val_batches = len(valset.dataset) / opt.batchSize + 1\n",
    "    if valset is not None:\n",
    "        real_loss = 0\n",
    "        for images, labels in valset:\n",
    "            if opt.cuda:\n",
    "                images = images.cuda(gpu_id)\n",
    "                labels = labels.cuda(gpu_id)\n",
    "        \n",
    "            images, labelv = Variable(images), Variable(labels)\n",
    "            \n",
    "            outputs = student(images)\n",
    "            real_loss += torch.nn.functional.cross_entropy(outputs, labelv).data.cpu().numpy()\n",
    "        real_loss = real_loss / real_val_batches\n",
    "        \n",
    "    return real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(testloader, net):\n",
    "    \"\"\"\n",
    "        Compute test accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    '''\n",
    "    if opt.cuda:\n",
    "        net = net.cuda()\n",
    "    '''\n",
    "    \n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        if opt.cuda:\n",
    "            images = images.cuda(gpu_id)\n",
    "            labels = labels.cuda(gpu_id)\n",
    "            \n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == (labels.long().view(-1) % opt.nClasses)).sum()\n",
    "\n",
    "    print('Accuracy of the network on test images: %f %%' % (100 * float(correct) / total))\n",
    "    return 100 * float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_update(params, p, use_grad_field=True):\n",
    "    init = True\n",
    "    for param in params:\n",
    "        if param is not None:\n",
    "            if init:\n",
    "                idx = torch.zeros_like(param, dtype=torch.bool)\n",
    "                idx.bernoulli_(1 - p)\n",
    "            if use_grad_field:\n",
    "                if param.grad is not None:\n",
    "                    idx = torch.zeros_like(param, dtype=torch.bool)\n",
    "                    idx.bernoulli_(1 - p)\n",
    "                    param.grad.data[idx] = 0\n",
    "            else:\n",
    "                init = False\n",
    "                param.data[idx] = 0\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, testloader, student, n_epochs=25, lr=0.0001, accountant=None):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "    #optimizer = optim.SGD(student.parameters(), lr=lr)\n",
    "    \n",
    "    if opt.cuda:\n",
    "        student = student.cuda(gpu_id)\n",
    "        criterion = criterion.cuda(gpu_id)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    num_batches = len(trainloader.dataset) / opt.batchSize + 1\n",
    "    sampling_prob = 1.0\n",
    "    \n",
    "    max_grad_norm = opt.C\n",
    "    sigma = opt.sigma * max_grad_norm\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda(gpu_id)\n",
    "                labels = labels.cuda(gpu_id)\n",
    "                \n",
    "            batch_size = float(len(inputs))\n",
    "            \n",
    "            inputv = Variable(inputs)\n",
    "            labelv = Variable(labels.long().view(-1) % opt.nClasses)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = student(inputv)\n",
    "            loss = criterion(outputs, labelv)\n",
    "            \n",
    "            if accountant:\n",
    "                grads_est = []\n",
    "                num_subbatch = 8\n",
    "                for j in range(num_subbatch):\n",
    "                    grad_sample = torch.autograd.grad(loss[np.delete(range(int(batch_size)), j)].mean(), [p for p in student.parameters() if p.requires_grad], retain_graph=True)\n",
    "                    with torch.no_grad():\n",
    "                        grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "                        #grad_sample /= max(1.0, grad_sample.norm().item() / max_grad_norm)\n",
    "                        grads_est += [grad_sample]\n",
    "                with torch.no_grad():\n",
    "                    sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "                    \n",
    "            (loss.mean()).backward()\n",
    "            running_loss += loss.mean().item()\n",
    "            \n",
    "            if accountant:\n",
    "                #torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "                for group in optimizer.param_groups:\n",
    "                    for p in group['params']:\n",
    "                        if p.grad is not None:\n",
    "                            p.grad.data += torch.randn_like(p.grad) * sigma #* max_grad_norm\n",
    "                sparsify_update(student.parameters(), p=sampling_prob)\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if accountant:\n",
    "                with torch.no_grad():\n",
    "                    q = batch_size / len(trainloader.dataset)\n",
    "                    # NOTE: \n",
    "                    # Using combinations within a set of gradients (like below)\n",
    "                    # does not actually produce samples from the correct distribution\n",
    "                    # (for that, we need to sample pairs of gradients independently).\n",
    "                    # However, the difference is not significant, and it speeds up computations.\n",
    "                    grad_pairs = list(zip(*itertools.combinations(grads_est, 2)))\n",
    "                    accountant.accumulate(\n",
    "                        ldistr=(torch.stack(grad_pairs[0]), opt.sigma*max_grad_norm),\n",
    "                        rdistr=(torch.stack(grad_pairs[1]), opt.sigma*max_grad_norm),\n",
    "                        q=q, \n",
    "                        steps=1,\n",
    "                    )\n",
    "                    running_eps = accountant.get_privacy(target_delta=1e-5)\n",
    "            \n",
    "        # print training stats every epoch\n",
    "        running_eps = accountant.get_privacy(target_delta=1e-5) if accountant else None\n",
    "        print(\"Epoch: %d/%d. Loss: %.3f. Privacy (𝜀,𝛿): %s\" %\n",
    "              (epoch + 1, n_epochs, running_loss / len(trainloader), running_eps))\n",
    "\n",
    "        student.eval()\n",
    "        acc = test(testloader, student)\n",
    "        accuracies += [acc]\n",
    "        student.train()\n",
    "        print(\"Test accuracy is %d %%\" % acc)\n",
    "        save_step = 100\n",
    "        if (epoch + 1) % save_step == 0:\n",
    "            torch.save(student.state_dict(), '%s/private_net_epoch_%d.pth' % (opt.outf, epoch + 1))\n",
    "            pickle.dump(accountant, open('%s/bayes_accountant_epoch_%d' % (opt.outf, epoch + 1), 'wb'))\n",
    "\n",
    "    print('Finished Training')\n",
    "    return student.cpu(), accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations applied to data\n",
    "transform = transforms.Compose([transforms.Resize(size=(128, 128)),\n",
    "                                transforms.RandomHorizontalFlip(), \n",
    "                                transforms.ColorJitter(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "transform_test = transforms.Compose([transforms.Resize(size=(128, 128)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# switch datasets\n",
    "if opt.dataset == 'mnist':\n",
    "    trainset = torchvision.datasets.MNIST(root=opt.dataroot + os.sep + opt.dataset, train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.MNIST(root=opt.dataroot + os.sep + opt.dataset, train=False, download=True, transform=transform)\n",
    "elif opt.dataset == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root=opt.dataroot + os.sep + opt.dataset, train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root=opt.dataroot + os.sep + opt.dataset, train=False, download=True, transform=transform_test)\n",
    "    trainset100 = torchvision.datasets.CIFAR100(root=opt.dataroot + os.sep + opt.dataset, train=True, download=True, transform=transform)\n",
    "    testset100 = torchvision.datasets.CIFAR100(root=opt.dataroot + os.sep + opt.dataset, train=False, download=True, transform=transform_test)\n",
    "elif opt.dataset == 'svhn':\n",
    "    trainset = torchvision.datasets.SVHN(root=opt.dataroot + os.sep + opt.dataset, split='train', download=True, transform=transform)\n",
    "    testset = torchvision.datasets.SVHN(root=opt.dataroot + os.sep + opt.dataset, split='test', download=True, transform=transform)\n",
    "    \n",
    "# initialise data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.batchSize, shuffle=True, num_workers=2, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=opt.batchSize, shuffle=False, num_workers=2)\n",
    "if opt.dataset == 'cifar10':\n",
    "    trainloader100 = torch.utils.data.DataLoader(trainset100, batch_size=opt.batchSize, shuffle=True, num_workers=2, drop_last=True)\n",
    "    testloader100 = torch.utils.data.DataLoader(testset100, batch_size=opt.batchSize, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train GAN and measure time\n",
    "start = time.time()\n",
    "\n",
    "netS = torchvision.models.vgg16(pretrained=True)\n",
    "netS.train()\n",
    "# Disable updates for feature extraction layers\n",
    "for p in netS.named_parameters():\n",
    "    p[1].requires_grad = False\n",
    "last_layers = [p for p in netS.parameters()][-2:]\n",
    "netS.classifier[-1] = nn.Linear(4096, 10)\n",
    "last_layers[-1] = nn.Parameter(last_layers[-1][:10])\n",
    "last_layers[-2] = nn.Parameter(last_layers[-2][:10])\n",
    "for ll in last_layers:\n",
    "    ll.requires_grad = True\n",
    "netS.aux_logits = False\n",
    "\n",
    "total_steps = opt.n_epochs * len(trainloader)\n",
    "bayes_accountant = BayesianPrivacyAccountant(powers=32, total_steps=total_steps)\n",
    "netS, accs = train(trainloader, testloader, netS, lr=0.001, n_epochs=opt.n_epochs, accountant=bayes_accountant)\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Time elapsed: %f\" % (stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epsilon = \", bayes_accountant.get_privacy(target_delta=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "netS.cuda(gpu_id)\n",
    "loss_fn.cuda(gpu_id)\n",
    "\n",
    "dists_train = []\n",
    "dists_test = []\n",
    "\n",
    "i = 0\n",
    "for inputs, labels in trainloader:\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "    inputs = inputs.cuda(gpu_id)\n",
    "    labels = labels.cuda(gpu_id)\n",
    "    netS.zero_grad()\n",
    "    outputs = netS(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    grads_est = []\n",
    "    num_subbatch = 100\n",
    "    for j in range(num_subbatch):\n",
    "        grad_sample = torch.autograd.grad(loss[np.delete(range(int(opt.batchSize)), j)].mean(), [p for p in netS.parameters() if p.requires_grad], retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "            grad_sample /= max(1.0, grad_sample.norm().item() / opt.C)\n",
    "            grads_est += [grad_sample]\n",
    "    with torch.no_grad():\n",
    "        grads_est = torch.stack(grads_est)\n",
    "        #sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "    q = opt.batchSize / len(trainloader.dataset)\n",
    "    dists_train += [pdist(grads_est.cpu())]\n",
    "    \n",
    "i = 0\n",
    "for inputs, labels in testloader:\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "    inputs = inputs.cuda(gpu_id)\n",
    "    labels = labels.cuda(gpu_id)\n",
    "    netS.zero_grad()\n",
    "    outputs = netS(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    grads_est = []\n",
    "    num_subbatch = 100\n",
    "    for j in range(num_subbatch):\n",
    "        grad_sample = torch.autograd.grad(loss[np.delete(range(int(opt.batchSize)), j)].mean(), [p for p in netS.parameters() if p.requires_grad], retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            grad_sample = torch.cat([g.view(-1) for g in grad_sample])\n",
    "            grad_sample /= max(1.0, grad_sample.norm().item() / opt.C)\n",
    "            grads_est += [grad_sample]\n",
    "    with torch.no_grad():\n",
    "        grads_est = torch.stack(grads_est)\n",
    "        #sparsify_update(grads_est, p=sampling_prob, use_grad_field=False)\n",
    "    q = opt.batchSize / len(trainloader.dataset)\n",
    "    dists_test += [pdist(grads_est.cpu())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_train = np.stack(dists_train).squeeze()\n",
    "dists_test = np.stack(dists_test).squeeze()\n",
    "\n",
    "plt.hist(dists_train.flatten(), bins=np.arange(0, 0.2, 0.005), label='Train', alpha=0.5)\n",
    "plt.hist(dists_test.flatten(), bins=np.arange(0, 0.2, 0.005), label='Test', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(r'Distance')\n",
    "plt.ylabel(r'Number of samples')\n",
    "plt.title(r'Pairwise gradient distances distribution, CIFAR10')\n",
    "plt.savefig('grad_dist_histogram_cifar10.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, ttest_ind, levene\n",
    "\n",
    "print(ttest_rel(dists_train.flatten(), dists_test.flatten()))\n",
    "print(ttest_ind(dists_train.flatten(), dists_test.flatten()))\n",
    "print(levene(dists_train.flatten(), dists_test.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
